{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:blue\" align=\"center\">Handwritten digits classification using neural network</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will classify handwritten digits using a simple neural network which has only input and output layers. We will than add a hidden layer and see how the performance of the model improves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-24 20:58:33.512526: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-24 20:58:33.512558: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-24 20:58:33.513446: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-24 20:58:33.519044: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-24 20:58:34.625151: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train) , (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f441a3af040>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "        0.07058824, 0.49411765, 0.53333333, 0.68627451, 0.10196078,\n",
       "        0.65098039, 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.11764706, 0.14117647,\n",
       "        0.36862745, 0.60392157, 0.66666667, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.88235294, 0.6745098 ,\n",
       "        0.99215686, 0.94901961, 0.76470588, 0.25098039, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.19215686, 0.93333333, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.98431373, 0.36470588, 0.32156863,\n",
       "        0.32156863, 0.21960784, 0.15294118, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.07058824, 0.85882353, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.77647059,\n",
       "        0.71372549, 0.96862745, 0.94509804, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.31372549, 0.61176471,\n",
       "        0.41960784, 0.99215686, 0.99215686, 0.80392157, 0.04313725,\n",
       "        0.        , 0.16862745, 0.60392157, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "        0.00392157, 0.60392157, 0.99215686, 0.35294118, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.54509804, 0.99215686, 0.74509804, 0.00784314,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.04313725, 0.74509804, 0.99215686, 0.2745098 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.1372549 , 0.94509804, 0.88235294,\n",
       "        0.62745098, 0.42352941, 0.00392157, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.31764706, 0.94117647,\n",
       "        0.99215686, 0.99215686, 0.46666667, 0.09803922, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.17647059,\n",
       "        0.72941176, 0.99215686, 0.99215686, 0.58823529, 0.10588235,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.0627451 , 0.36470588, 0.98823529, 0.99215686, 0.73333333,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.97647059, 0.99215686, 0.97647059,\n",
       "        0.25098039, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.18039216,\n",
       "        0.50980392, 0.71764706, 0.99215686, 0.99215686, 0.81176471,\n",
       "        0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.15294118, 0.58039216, 0.89803922,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.98039216, 0.71372549,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.09411765, 0.44705882, 0.86666667, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.78823529, 0.30588235, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.09019608, 0.25882353,\n",
       "        0.83529412, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.77647059, 0.31764706, 0.00784314, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.07058824, 0.67058824, 0.85882353, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.76470588, 0.31372549,\n",
       "        0.03529412, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.21568627,\n",
       "        0.6745098 , 0.88627451, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.95686275, 0.52156863, 0.04313725, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.53333333,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.83137255, 0.52941176,\n",
       "        0.51764706, 0.0627451 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flattened = X_train.reshape(len(X_train), 28*28)\n",
    "X_test_flattened = X_test.reshape(len(X_test), 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_flattened.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "       0.07058824, 0.49411765, 0.53333333, 0.68627451, 0.10196078,\n",
       "       0.65098039, 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.11764706, 0.14117647, 0.36862745, 0.60392157,\n",
       "       0.66666667, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.88235294, 0.6745098 , 0.99215686, 0.94901961,\n",
       "       0.76470588, 0.25098039, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.19215686, 0.93333333,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.98431373, 0.36470588,\n",
       "       0.32156863, 0.32156863, 0.21960784, 0.15294118, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.07058824, 0.85882353, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.77647059, 0.71372549,\n",
       "       0.96862745, 0.94509804, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.31372549, 0.61176471, 0.41960784, 0.99215686, 0.99215686,\n",
       "       0.80392157, 0.04313725, 0.        , 0.16862745, 0.60392157,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "       0.00392157, 0.60392157, 0.99215686, 0.35294118, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.54509804,\n",
       "       0.99215686, 0.74509804, 0.00784314, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.04313725, 0.74509804, 0.99215686,\n",
       "       0.2745098 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.1372549 , 0.94509804, 0.88235294, 0.62745098,\n",
       "       0.42352941, 0.00392157, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.31764706, 0.94117647, 0.99215686, 0.99215686, 0.46666667,\n",
       "       0.09803922, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.17647059,\n",
       "       0.72941176, 0.99215686, 0.99215686, 0.58823529, 0.10588235,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.0627451 , 0.36470588,\n",
       "       0.98823529, 0.99215686, 0.73333333, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.97647059, 0.99215686,\n",
       "       0.97647059, 0.25098039, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.18039216, 0.50980392,\n",
       "       0.71764706, 0.99215686, 0.99215686, 0.81176471, 0.00784314,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.15294118,\n",
       "       0.58039216, 0.89803922, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.98039216, 0.71372549, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.09411765, 0.44705882, 0.86666667, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.78823529, 0.30588235, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.09019608, 0.25882353, 0.83529412, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.77647059, 0.31764706,\n",
       "       0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.07058824, 0.67058824, 0.85882353,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.76470588,\n",
       "       0.31372549, 0.03529412, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.21568627, 0.6745098 ,\n",
       "       0.88627451, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.95686275, 0.52156863, 0.04313725, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.53333333, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.83137255, 0.52941176, 0.51764706, 0.0627451 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_flattened[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style='color:purple'>Very simple neural network with no hidden layers</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-24 20:58:39.410292: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-24 20:58:39.601969: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-24 20:58:39.602162: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-24 20:58:39.603243: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-24 20:58:39.603393: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-24 20:58:39.603515: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-24 20:58:39.671934: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-24 20:58:39.672158: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-24 20:58:39.672308: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-24 20:58:39.672710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2284 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2024-01-24 20:58:40.531487: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 188160000 exceeds 10% of free system memory.\n",
      "2024-01-24 20:58:40.777769: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-24 20:58:41.440426: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f4312d75e00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-24 20:58:41.440477: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "2024-01-24 20:58:41.466756: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-01-24 20:58:42.833437: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:447] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.9.4.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2024-01-24 20:58:42.836284: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at xla_ops.cc:574 : FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.\n",
      "2024-01-24 20:58:42.855315: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:447] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.9.4.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2024-01-24 20:58:42.857303: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at xla_ops.cc:574 : FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Graph execution error:\n\nDetected at node Adam/StatefulPartitionedCall defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n\n  File \"/tmp/ipykernel_114916/4231089507.py\", line 9, in <module>\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1154, in train_step\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 544, in minimize\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 1223, in apply_gradients\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 652, in apply_gradients\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 1253, in _internal_apply_gradients\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 1345, in _distributed_apply_gradients_fn\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 1340, in apply_grad_to_update_var\n\nDNN library initialization failed. Look at the errors above for more details.\n\t [[{{node Adam/StatefulPartitionedCall}}]] [Op:__inference_train_function_508]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[1;32m      2\u001b[0m     keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m10\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m784\u001b[39m,), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m ])\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m               loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_flattened\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Graph execution error:\n\nDetected at node Adam/StatefulPartitionedCall defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n\n  File \"/tmp/ipykernel_114916/4231089507.py\", line 9, in <module>\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1154, in train_step\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 544, in minimize\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 1223, in apply_gradients\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 652, in apply_gradients\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 1253, in _internal_apply_gradients\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 1345, in _distributed_apply_gradients_fn\n\n  File \"/home/peter/.local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 1340, in apply_grad_to_update_var\n\nDNN library initialization failed. Look at the errors above for more details.\n\t [[{{node Adam/StatefulPartitionedCall}}]] [Op:__inference_train_function_508]"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(10, input_shape=(784,), activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_flattened, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_test_flattened, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = model.predict(X_test_flattened)\n",
    "y_predicted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(X_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**np.argmax finds a maximum element  from an array and returns the index of it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(y_predicted[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_labels = [np.argmax(i) for i in y_predicted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = tf.math.confusion_matrix(labels=y_test,predictions=y_predicted_labels)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install seaborn\n",
    "# Note: \n",
    "# Below command install seaborn insisde the base environment. It does not work for kernel\n",
    "# !pip install seabron \n",
    "# use \"%pip install seabron\". It work for jupuyter notebook kenrne\n",
    "# %pip install seaborn\n",
    "import seaborn as sn\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style='color:purple'>Using hidden layer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(100, input_shape=(784,), activation='relu'),\n",
    "    keras.layers.Dense(10, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_flattened, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = model.predict(X_test_flattened)\n",
    "y_predicted_labels = [np.argmax(i) for i in y_predicted]\n",
    "cm = tf.math.confusion_matrix(labels=y_test,predictions=y_predicted_labels)\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test_flattened,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style='color:purple'>Using Flatten layer so that we don't have to call .reshape on input dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
